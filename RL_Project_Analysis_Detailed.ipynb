{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d02c92",
   "metadata": {},
   "source": [
    "\n",
    "# Analysis of the Final Learned Policy for a Q‑Learning Agent\n",
    "**AI Studio: Project Analysis — Penalty Shootout**\n",
    "\n",
    "This notebook provides a detailed qualitative and quantitative analysis of the final learned policy of the **Q‑learning** agent for the **Penalty Shootout** game. Since episodes are short and rewards are discrete (goal vs save), this analysis focuses on inspecting the **Q‑table** to understand the agent's strategy, strengths, and limitations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6be83",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup and Data Loading\n",
    "First, import the necessary libraries and load the final **Q‑table** and training logs from the saved file `training_data.npz`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fc418d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Requirements\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# pip install numpy matplotlib  # if running in a fresh environment\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Requirements\n",
    "# pip install numpy matplotlib  # if running in a fresh environment\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "TRAINING_FILE = \"training_data.npz\"\n",
    "\n",
    "# Load the Q table\n",
    "try:\n",
    "    data = np.load(TRAINING_FILE, allow_pickle=True)\n",
    "    Q = data[\"Q\"]\n",
    "    returns = data[\"returns\"]\n",
    "    moving_avg = data[\"moving_avg\"]\n",
    "    epsilons = data[\"epsilons\"]\n",
    "    ACTIONS = list(data[\"actions\"])\n",
    "    print(f\"Loaded: {TRAINING_FILE}\")\n",
    "    print(\"Q-table shape:\", Q.shape)\n",
    "except FileNotFoundError:\n",
    "    # Fallback mock so the notebook is still runnable for formatting preview\n",
    "    print(\"Could not find 'training_data.npz'. Creating a small mock Q-table for preview...\")\n",
    "    ACTIONS = [\"L\", \"LC\", \"C\", \"RC\", \"R\"]\n",
    "    # (kick_idx 0..5) × (score_diff -5..+5) = (6 * 11) states; 5 actions\n",
    "    Q = np.random.rand(6*11, 5).astype(np.float32)\n",
    "    returns = np.random.rand(500).astype(np.float32) * 5.0\n",
    "    moving_avg = np.convolve(returns, np.ones(50)/50, mode=\"same\")\n",
    "    epsilons = np.linspace(1.0, 0.05, len(returns)).astype(np.float32)\n",
    "\n",
    "print(\"Episodes:\", len(returns))\n",
    "print(\"Actions:\", ACTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# State index mapping used in training:\n",
    "# idx = (kick_idx * cols) + (diff + max_diff), where cols = 2*max_diff + 1, and max_diff = kicks\n",
    "KICKS = 5\n",
    "MAX_DIFF = KICKS\n",
    "COLS = 2 * MAX_DIFF + 1\n",
    "ROWS = KICKS + 1  # kick_idx 0..5\n",
    "\n",
    "def idx_from_state(kick_idx, score_diff):\n",
    "    return (kick_idx * COLS) + (score_diff + MAX_DIFF)\n",
    "\n",
    "def state_from_idx(idx):\n",
    "    kick_idx = idx // COLS\n",
    "    diff = (idx % COLS) - MAX_DIFF\n",
    "    return kick_idx, diff\n",
    "\n",
    "score_labels = [str(d) for d in range(-MAX_DIFF, MAX_DIFF+1)]\n",
    "kick_labels = [str(k) for k in range(ROWS)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c2e0c0",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Learning Curves\n",
    "We start with **episode goals** and a **moving average** to verify that the agent improved, followed by the **ε‑greedy schedule** used during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17616bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(returns, label=\"Episode goals\")\n",
    "plt.plot(moving_avg, label=\"Moving average (50)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Goals per episode\")\n",
    "plt.title(\"Learning Curve — Goals\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epsilons)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon (ε)\")\n",
    "plt.title(\"Exploration Schedule\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e764c",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Qualitative Analysis of the Final Learned Policy\n",
    "To understand the agent’s preferences, we visualize its **greedy action** (argmax over Q) across relevant state slices.  \n",
    "Because the state is `(kick_idx, score_diff)`, we examine:\n",
    "- **Greedy action vs. score difference** for a *fixed kick index*; and\n",
    "- **Greedy action vs. kick index** for a *fixed score difference*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a matrix: rows = kick_idx (0..5), cols = score_diff (-5..+5), values = greedy action index\n",
    "greedy = np.argmax(Q, axis=1)\n",
    "grid = np.zeros((ROWS, COLS), dtype=int)\n",
    "for k in range(ROWS):\n",
    "    for d in range(-MAX_DIFF, MAX_DIFF+1):\n",
    "        grid[k, d+MAX_DIFF] = greedy[idx_from_state(k, d)]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(grid, aspect=\"auto\")\n",
    "plt.xticks(ticks=range(COLS), labels=score_labels, rotation=0)\n",
    "plt.yticks(ticks=range(ROWS), labels=kick_labels)\n",
    "plt.xlabel(\"Score Difference (goals - saves)\")\n",
    "plt.ylabel(\"Kick Index (0: first ... 5: last)\")\n",
    "plt.title(\"Greedy Action Map (indices) across State Space\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Legend (action index → label):\", {i:a for i,a in enumerate(ACTIONS)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6dd04",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Preferred Actions by Score Context (Fixed Kick)\n",
    "Here we pick a specific kick (e.g., **kick 2**) and show the **Q‑values** for all actions across score differences. This highlights how the policy changes when trailing vs. leading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589865c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_k = 2  # choose any 0..5\n",
    "M = np.zeros((len(ACTIONS), COLS), dtype=float)\n",
    "for a in range(len(ACTIONS)):\n",
    "    for d in range(-MAX_DIFF, MAX_DIFF+1):\n",
    "        M[a, d+MAX_DIFF] = Q[idx_from_state(fixed_k, d), a]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(M, aspect=\"auto\")\n",
    "plt.yticks(ticks=range(len(ACTIONS)), labels=ACTIONS)\n",
    "plt.xticks(ticks=range(COLS), labels=score_labels, rotation=0)\n",
    "plt.xlabel(\"Score Difference\")\n",
    "plt.ylabel(\"Action\")\n",
    "plt.title(f\"Q-values by Score Difference at Kick {fixed_k}\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353a501",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Preferred Actions over the Shootout Timeline (Fixed Score)\n",
    "Now we fix a **score difference** (e.g., **0**) and plot **Q‑values** across **kick index**. This shows whether the agent becomes more/less aggressive as the final kick approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_diff = 0  # choose from -5..+5\n",
    "M2 = np.zeros((len(ACTIONS), ROWS), dtype=float)\n",
    "for a in range(len(ACTIONS)):\n",
    "    for k in range(ROWS):\n",
    "        M2[a, k] = Q[idx_from_state(k, fixed_diff), a]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(M2, aspect=\"auto\")\n",
    "plt.yticks(ticks=range(len(ACTIONS)), labels=ACTIONS)\n",
    "plt.xticks(ticks=range(ROWS), labels=kick_labels)\n",
    "plt.xlabel(\"Kick Index\")\n",
    "plt.ylabel(\"Action\")\n",
    "plt.title(f\"Q-values by Kick Index at Score Diff {fixed_diff}\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123ef6d",
   "metadata": {},
   "source": [
    "\n",
    "### 3.3 Unvisited or Undertrained States\n",
    "We flag any **state rows** in the Q‑table that appear to have remained near zero (common if the agent rarely reaches those states during training).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1679284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_norms = np.linalg.norm(Q, axis=1)\n",
    "low_rows = np.where(row_norms < 1e-6)[0]\n",
    "print(f\"Total states: {Q.shape[0]} | Near-zero rows: {len(low_rows)}\")\n",
    "\n",
    "if len(low_rows) > 0:\n",
    "    example = low_rows[:10]\n",
    "    decoded = [state_from_idx(i) for i in example]\n",
    "    print(\"Example near-zero state indices (kick_idx, score_diff):\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec51d228",
   "metadata": {},
   "source": [
    "\n",
    "## 4. High-Level Policy: Overall Action Preference\n",
    "We compute the **overall greedy action distribution** across the entire state space. This provides a high-level view of preferred shot directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41333daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vals, counts = np.unique(greedy, return_counts=True)\n",
    "labels = [ACTIONS[v] for v in vals]\n",
    "\n",
    "plt.figure()\n",
    "# Donut chart using a pie with a white circle\n",
    "plt.pie(counts, labels=labels, autopct=\"%1.1f%%\", startangle=90)\n",
    "centre_circle = plt.Circle((0,0), 0.65, fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "plt.title(\"Overall Greedy Action Distribution (All States)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a611230",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Optional Quantitative Evaluation\n",
    "Run multiple evaluation episodes using the learned **greedy policy** (no exploration) to report mean ± std goals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf11b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_optional_eval():\n",
    "    try:\n",
    "        from penalty_shootout_ql import PenaltyShootoutEnv, to_state_index\n",
    "    except Exception as e:\n",
    "        print(\"Env not available, skipping evaluation:\", e)\n",
    "        return None\n",
    "\n",
    "    env = PenaltyShootoutEnv()\n",
    "    max_diff = env.kicks\n",
    "\n",
    "    def policy(state):\n",
    "        idx = to_state_index(state, env.kicks, max_diff)\n",
    "        return int(np.argmax(Q[idx]))\n",
    "\n",
    "    def evaluate(n_episodes=100):\n",
    "        scores = []\n",
    "        for _ in range(n_episodes):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            total = 0.0\n",
    "            while not done:\n",
    "                a = policy(s)\n",
    "                s, r, done, _ = env.step(a)\n",
    "                total += r\n",
    "            scores.append(total)\n",
    "        return float(np.mean(scores)), float(np.std(scores))\n",
    "\n",
    "    mean_score, std_score = evaluate(200)\n",
    "    print(f\"Evaluation over 200 episodes: {mean_score:.2f} ± {std_score:.2f} goals\")\n",
    "\n",
    "\n",
    "run_optional_eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848ca99",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Conclusions\n",
    "- The **learning curves** confirm improvement as ε decays and the moving average of goals increases.\n",
    "\n",
    "- The **greedy action maps** reveal how the agent adapts to **score pressure** and **late kicks**.\n",
    "\n",
    "- Any **near‑zero Q rows** highlight states that were rarely or never encountered; these are candidates for curriculum tweaks or reward shaping.\n",
    "\n",
    "- The **overall action distribution** summarizes the agent’s style (e.g., favoring safer corners or central shots), which you can discuss in the report.\n",
    "\n",
    "\n",
    "\n",
    "**Next steps:** compare with **SARSA**, modify the goalkeeper distribution, extend the action set (e.g., shot power), or switch to **DQN** with function approximation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
